{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8635fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 3), ('box', 3)]\n"
     ]
    }
   ],
   "source": [
    "senteces = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "sentences = word_tokenize(senteces)\n",
    "sentences = [item.lower() for item in sentences]  # lowercase\n",
    "sentences = [item for item in sentences if item.isalpha()]  # remove punctuation\n",
    "senteces = [item for item in sentences if item not in stopwords_list]  # stopwords\n",
    "\n",
    "c = Counter(senteces)\n",
    "# print(c)\n",
    "# print(c['The'])\n",
    "print(c.most_common(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d58c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': 0, 'can': 1, 'creative': 2, 'explains': 3, 'follow': 4, 'great': 5, 'ideas': 6, 'important': 7, 'is': 8, 'nearly': 9, 'one': 10, 'possess': 11, 'process': 12, 'similar': 13, 'skills': 14, 'the': 15, 'thinking': 16, 'this': 17, 'understanding': 18, 'useful': 19, 'works': 20, '.': 21, 'always': 22, 'code': 23, 'computer': 24, 'doing': 25, 'faster': 26, 'no': 27, 'of': 28, 'old': 29, 'programming': 30, 'remember': 31, 'reminds': 32, 'saying': 33, 'something': 34, 'statement': 35, 'there': 36, ',': 37, '5': 38, '70': 39, 'a': 40, 'become': 41, 'career': 42, 'case': 43, 'creativity': 44, 'end': 45, 'field': 46, 'held': 47, 'his': 48, 'innovation': 49, 'key': 50, 'patents': 51, 'photography': 52, 'share': 53, 'steps': 54, 'story': 55, 'study': 56, 'to': 57, 'trailblazer': 58, 'went': 59, 'which': 60, 'will': 61, 'and': 62, 'cameras': 63, 'decade': 64, 'experimenting': 65, 'learning': 66, 'new': 67, 'optics': 68, 'printers': 69, 'rest': 70, 'spent': 71, 'techniques': 72}\n",
      "[[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1)], [(21, 1), (22, 1), (23, 2), (24, 1), (25, 2), (26, 2), (27, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1)], [(2, 1), (12, 1), (15, 3), (18, 1), (19, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)], [(15, 1), (52, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1)]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "my_documents = [\n",
    "    'Nearly all great ideas follow a similar creative process and this article explains how this process works. Understanding this is important because creative thinking is one of the most useful skills you can possess.',\n",
    "    'Not doing something will always be faster than doing it. This statement reminds me of the old computer programming saying, Remember that there is no code faster than no code.',\n",
    "    'He went on to become a trailblazer in the field of photography and held over 70 patents by the end of his career. His story of creativity and innovation, which I will share now, is a useful case study for understanding the 5 key steps of the creative process.',\n",
    "    'He spent the rest of the decade experimenting with new photography techniques and learning about cameras, printers, and optics.',   \n",
    "]\n",
    "\n",
    "tokenized_docs = [word_tokenize(article.lower()) for article in my_documents]\n",
    "for article in tokenized_docs:\n",
    "    for item in article:\n",
    "        if not item.isalpha() or item in stopwords_list:\n",
    "            article.remove(item)\n",
    "\n",
    "\n",
    "# Słownik (mapa)\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# Korpus\n",
    "corpus = [dictionary.doc2bow(article) for article in tokenized_docs]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "951e1771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.21544988363399623), (1, 0.21544988363399623), (2, 0.21544988363399623), (3, 0.21544988363399623), (4, 0.21544988363399623), (5, 0.21544988363399623), (6, 0.21544988363399623), (7, 0.21544988363399623), (8, 0.21544988363399623), (9, 0.21544988363399623), (10, 0.21544988363399623), (11, 0.21544988363399623), (12, 0.21544988363399623), (13, 0.21544988363399623), (14, 0.21544988363399623), (15, 0.04470989046168583), (16, 0.21544988363399623), (17, 0.43089976726799245), (18, 0.10772494181699811), (19, 0.10772494181699811), (20, 0.21544988363399623)]\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf_corpus = TfidfModel(corpus)\n",
    "print(tfidf_corpus[corpus[0]])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332fb2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jerem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a2d466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  and/CC\n",
      "  (GPE New/NNP York/NNP)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "# ner\n",
    "\n",
    "import nltk\n",
    "\n",
    "sentence = \"European and New York authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices\"\n",
    "\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "\n",
    "tagged_tokens = nltk.pos_tag(tokenized_sent)\n",
    "print(nltk.ne_chunk(tagged_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37568f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(European, New York, Google, $5.1 billion, Wednesday)\n",
      "European NORP\n",
      "New York GPE\n",
      "Google ORG\n",
      "$5.1 billion MONEY\n",
      "Wednesday DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.get_pipe(\"ner\")\n",
    "\n",
    "doc = nlp(sentence)\n",
    "print(doc.ents)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent} {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce889788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# klasyfikacja w nlp\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('fake_or_real.csv')\n",
    "print(df.head())\n",
    "\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, Y_train, T_test = train_test_split(\n",
    "    df['text'],\n",
    "    y,\n",
    "    test_size=0.33,\n",
    "    random_state=53\n",
    ")\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english') # model BOW\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "# BOW\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "print(count_vectorizer.get_feature_names()[0:10])\n",
    "\n",
    "print(count_train.A[:10])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
